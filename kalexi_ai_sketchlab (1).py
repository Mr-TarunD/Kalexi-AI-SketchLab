# -*- coding: utf-8 -*-
"""Kalexi-AI SketchLab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169GVC6Vyz7dvQifNpnMe52LKTmvD6bZS
"""

# Cell 1: Install Optimized Dependencies (Ensure this runs first)

!pip install streamlit -q
!pip install controlnet_aux -q
!pip install torch torchvision -q
!pip install opencv-python-headless -q
!npm install localtunnel -q

# Commented out IPython magic to ensure Python compatibility.
# # Cell 2: Write the Streamlit Application (app.py)
# 
# %%writefile app.py
# import streamlit as st
# import numpy as np
# from PIL import Image
# import torch
# import cv2
# from controlnet_aux import HEDdetector, PidiNetDetector
# import time
# 
# # --- Page Config ---
# st.set_page_config(page_title="Kalexi-AI Lab", layout="wide", page_icon="‚ö°")
# 
# # --- CSS for Status Indicators ---
# st.markdown("""
# <style>
#     .main { background-color: #0E1117; }
#     .status-box {
#         padding: 10px;
#         border-radius: 5px;
#         margin-bottom: 10px;
#         font-weight: bold;
#     }
#     .status-good { background-color: #1c4f2e; color: #4caf50; border: 1px solid #4caf50; }
#     .status-bad { background-color: #521818; color: #ff5252; border: 1px solid #ff5252; }
# </style>
# """, unsafe_allow_html=True)
# 
# st.title("‚ö° Kalexi-AI SketchLab")
# 
# # --- SIDEBAR: SYSTEM DIAGNOSTICS (NEW) ---
# with st.sidebar:
#     st.header("üñ•Ô∏è System Status")
# 
#     # CHECK 1: IS GPU ON?
#     if torch.cuda.is_available():
#         gpu_name = torch.cuda.get_device_name(0)
#         st.markdown(f"""
#         <div class="status-box status-good">
#         ‚úÖ GPU DETECTED<br>
#         <small>{gpu_name}</small>
#         </div>
#         """, unsafe_allow_html=True)
#         device_status = "fast"
#     else:
#         st.markdown("""
#         <div class="status-box status-bad">
#         ‚ö†Ô∏è CPU DETECTED (SLOW)<br>
#         <small>Enable T4 GPU in Colab Settings</small>
#         </div>
#         """, unsafe_allow_html=True)
#         device_status = "slow"
# 
#     st.divider()
# 
#     mode = st.radio("Mode", ["‚ö° Instant (No AI)", "üß† GenAI (High Quality)"])
# 
# # --- FUNCTIONS ---
# def fast_sketch(img):
#     """Instant OpenCV sketch"""
#     img_np = np.array(img)
#     gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
#     inverted = 255 - gray
#     blurred = cv2.GaussianBlur(inverted, (21, 21), 0)
#     return cv2.divide(gray, 255 - blurred, scale=256)
# 
# @st.cache_resource
# def load_models():
#     """Loads models to GPU"""
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     hed = HEDdetector.from_pretrained("lllyasviel/Annotators").to(device)
#     pidi = PidiNetDetector.from_pretrained("lllyasviel/Annotators").to(device)
#     return hed, pidi
# 
# # --- MAIN APP ---
# col1, col2 = st.columns(2)
# 
# with col1:
#     uploaded_file = st.file_uploader("Upload Image", type=['jpg', 'png', 'jpeg'])
#     if uploaded_file:
#         image = Image.open(uploaded_file).convert("RGB")
#         st.image(image, caption="Original", use_container_width=True)
# 
# with col2:
#     if uploaded_file:
#         # Resize logic to keep it fast even on 4K images
#         w, h = image.size
#         if max(w, h) > 600:
#             ratio = 600 / max(w, h)
#             image = image.resize((int(w*ratio), int(h*ratio)))
# 
#         if mode == "‚ö° Instant (No AI)":
#             # Takes 0.05 seconds
#             start = time.time()
#             result = fast_sketch(image)
#             st.image(result, caption=f"Instant Result ({round(time.time()-start, 3)}s)", use_container_width=True)
# 
#         elif mode == "üß† GenAI (High Quality)":
#             if device_status == "slow":
#                 st.warning("‚ö†Ô∏è You are on CPU. This will take ~45 seconds. Switch to GPU for ~3s speeds.")
# 
#             if st.button("Generate Sketch"):
#                 with st.spinner("Loading AI & Processing... (First run takes longer)"):
#                     hed, pidi = load_models()
#                     result = pidi(image, safe=True) # PIDI is usually faster/cleaner
#                     st.image(result, caption="GenAI Result", use_container_width=True)

# Cell 3: Launch and Tunneling Commands

import urllib

# 1. Command to fetch and print the external IP address. This IP is required by localtunnel as a security password.
print("‚ö†Ô∏è COPY THIS PASSWORD:", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

# 2. Command to run the Streamlit application ('app.py') in the background on port 8501, redirecting output to a log file.
!streamlit run app.py &>/content/logs.txt &

# 3. Command to create a public URL tunnel to the running Streamlit app on port 8501.
!npx localtunnel --port 8501

